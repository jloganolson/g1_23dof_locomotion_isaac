# G1 23-DOF Locomotion with Isaac Lab

This repository contains my first stab at training a locomotion policy for the Unitree G1 (23DOF) using Isaac Lab. I also implemented symmetry loss and removed the phase-based rewards. First stab at sim2sim deployment using Mujoco is also included.

<video src="g1-23dof-isaac-lab-current.mp4" controls width="720"></video>

The following readme.md was generated by Cursor - please forgive its slop vibe

## ğŸš€ Quick Start

### Training
```bash
# Basic training
python scripts/rsl_rl/train.py --task=Loco --headless --logger wandb
```

### Evaluation
```bash
# Play trained policy
python scripts/rsl_rl/play.py --task Loco --num_envs 32
```

### Parameter Sweep
```bash
# Run parameter sweep with automatic analysis
python sweep_script_with_analysis.py
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ scripts/rsl_rl/           # Training and evaluation scripts
â”‚   â”œâ”€â”€ train.py             # Main training script
â”‚   â”œâ”€â”€ play.py              # Policy evaluation script
â”‚   â””â”€â”€ cli_args.py          # Command line argument definitions
â”œâ”€â”€ deployment/              # Sim2sim deployment
â”‚   â”œâ”€â”€ deploy_sim.py        # MuJoCo deployment script
â”‚   â”œâ”€â”€ policy.pt            # Trained policy weights
â”‚   â””â”€â”€ g1_description/      # Robot description files
â”œâ”€â”€ source/                  # Isaac Lab environment source
â”‚   â””â”€â”€ g1_23dof_locomotion_isaac/
â”‚       â””â”€â”€ tasks/manager_based/g1_23dof_locomotion_isaac/
â”‚           â”œâ”€â”€ g1_23dof_locomotion_isaac_env_cfg.py  # Environment configuration
â”‚           â”œâ”€â”€ agents/rsl_rl_ppo_cfg.py              # PPO agent configuration
â”‚           â””â”€â”€ events.py                             # Domain randomization events
â”œâ”€â”€ sweep_script_with_analysis.py  # Parameter sweep automation
â”œâ”€â”€ sweep_analyzer.py              # Sweep results analysis
â””â”€â”€ outputs/                      # Training outputs and logs
```

## ğŸ”§ Configuration

### Experiment Setup
When running `sweep_script_with_analysis.py`, ensure the experiment name is consistent across:
- `sweep_script_with_analysis.py` (line 8)
- `source/g1_23dof_locomotion_isaac/g1_23dof_locomotion_isaac/tasks/manager_based/g1_23dof_locomotion_isaac/agents/rsl_rl_ppo_cfg.py`

### Current Experiment
- **Experiment Name**: `g1_23dof_sweep_v16`
- **Resume from Run**: 27 (configurable in sweep script)

### Current Parameter Sweep
The active sweep investigates:
- `env.rewards.feet_air_time.weight`: [1.0, 3.0, 5.0]
- `env.rewards.both_feet_air.weight`: [-0.1, -0.5, 0.0]
- `env.rewards.action_rate_l2.weight`: [-0.005, -0.01]
- `env.rewards.joint_deviation_arms.weight`: [-0.1, -0.2]


## ğŸ® Deployment

### Sim2Sim Deployment
The `deployment/` folder contains MuJoCo-based deployment scripts:

```bash
cd deployment
python deploy_sim.py
```

### Keyboard Control
- **WASD**: Forward/backward/left/right movement
- **Q/E**: Yaw rotation
- **Arrow Keys**: Alternative control scheme

## ğŸš§ Current Challenges
1. **Domain Randomization**: Current sim2sim performance is poor - need to significantly expand randomization parameters (friction, mass, motor gains, terrain properties) to improve transfer robustness
2. **Real-time Policy Control**: Need to implement real-time policy switching/control in Isaac Sim to verify if poor performance is due to sim2sim gap vs human control limitations


## ğŸ“ License

This project is based on the Isaac Lab framework and follows the same licensing terms.

---

**Note**: This is a work in progress. The current implementation focuses on symmetry-based learning approaches and requires further domain randomization work for robust sim2sim deployment. 
